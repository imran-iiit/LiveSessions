{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Magic Commands\n",
    "Magic commands (those that start with `%`) are commands that modify a configuration of Jupyter Notebooks. A number of magic commands are available by default (see list [here](http://ipython.readthedocs.io/en/stable/interactive/magics.html))--and many more can be added with extensions. The magic command added in this section allows `matplotlib` to display our plots directly on the browser instead of having to save them on a local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 7: Optimizing a deep learning model\n",
    "In this activity we optimize our deep learning model. We aim achieve greater performance than our model `bitcoin_lstm_v0`, which is off at about **8.4%** from the real Bitcoin prices. We explore the following topics in this notebook:\n",
    "\n",
    "* Experimenting with different layers and the number of nodes\n",
    "* Grid search strategy for epoch and activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers.core import Dense, Activation, Dropout, ActivityRegularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utilities import (create_groups, split_lstm_input, \n",
    "                               train_model, plot_two_series, rmse, \n",
    "                               mape, denormalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "We will load our same train and testing set from previous activitites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = create_groups(\n",
    "    train['close_point_relative_normalization'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = create_groups(\n",
    "    test['close_point_relative_normalization'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = split_lstm_input(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Model\n",
    "For reference, let's load data for `v0` of our model and train it alongside future modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v0 = load_model('bitcoin_lstm_v0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_model(model=model_v0, X=X_train, Y=Y_train, epochs=100, version=0, run_number=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Layers and Nodes\n",
    "We can modify our model to include other layers now. When using LSTM cells, one typically adds other LSTM layer in a sequence. In our case the layer will have the same number of neurons as the original layer.\n",
    "\n",
    "In order for this to work, however, we need to modify the parameter `return_sequences` to `True` on the first LSTM layer. We do this because the first layer expects a sequence of data with the same as input that the of the first layer. When this parameter is set to `False` the LSTM layer outputs the predicted parameters in a different incompatible output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_length = 7\n",
    "number_of_periods = 76\n",
    "batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1 = Sequential()\n",
    "model_v1.add(LSTM(\n",
    "    units=period_length,\n",
    "    batch_input_shape=(batch_size, number_of_periods, period_length),\n",
    "    input_shape=(number_of_periods, period_length),\n",
    "    return_sequences=True, stateful=False))\n",
    "\n",
    "#\n",
    "#  Add new LSTM layer to this network here.\n",
    "#\n",
    "\n",
    "model_v1.add(Dense(units=period_length))\n",
    "model_v1.add(Activation(\"linear\"))\n",
    "\n",
    "model_v1.compile(loss=\"mse\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_model(model=model_v1, X=X_train, Y=Y_train, epochs=100, version=1, run_number=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs\n",
    "Epochs are the number of times the network adjust its weights in response to data passing through and its loss function. Running a model for more epochs can allow it to learn more from data, but you also run the risk of overfitting.\n",
    "\n",
    "When training a model, prefer to increase the epochs exponentially until the loss function starts to plateau. In the case of the `bitcoin_lstm_v0` model, its loss function plateaus at about 100 epochs. If one attempts to train it at 10^3 epochs, the model barely gains any improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  Change the number of epochs below\n",
    "#  to a higher number (try 10**3) and\n",
    "#  evaluate the results on TensorBoard.\n",
    "#\n",
    "number_of_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2 = Sequential()\n",
    "model_v2.add(LSTM(\n",
    "    units=period_length,\n",
    "    batch_input_shape=(batch_size, number_of_periods, period_length),\n",
    "    input_shape=(number_of_periods, period_length),\n",
    "    return_sequences=True, stateful=False))\n",
    "\n",
    "model_v2.add(LSTM(\n",
    "    units=period_length,\n",
    "    batch_input_shape=(batch_size, number_of_periods, period_length),\n",
    "    input_shape=(number_of_periods, period_length),\n",
    "    return_sequences=False, stateful=False))\n",
    "\n",
    "model_v2.add(Dense(units=period_length))\n",
    "model_v2.add(Activation(\"linear\"))\n",
    "\n",
    "model_v2.compile(loss=\"mse\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_model(model=model_v2, X=X_train, Y=Y_train, epochs=number_of_epochs, \n",
    "            version=2, run_number=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "Due to its non-linear properties and efficient computation, we will use the `relu` function as this network's activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  Instead of using a ReLU, visit\n",
    "#  the Keras official documentation (https://keras.io/activations/)\n",
    "#  and choose a different function to try (maybe \"tanh\").\n",
    "#\n",
    "activation_function = \"linear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v3 = Sequential()\n",
    "model_v3.add(LSTM(\n",
    "    units=period_length,\n",
    "    batch_input_shape=(batch_size, number_of_periods, period_length),\n",
    "    input_shape=(number_of_periods, period_length),\n",
    "    return_sequences=True, stateful=False))\n",
    "\n",
    "model_v3.add(LSTM(\n",
    "    units=period_length,\n",
    "    batch_input_shape=(batch_size, number_of_periods, period_length),\n",
    "    input_shape=(number_of_periods, period_length),\n",
    "    return_sequences=False, stateful=False))\n",
    "\n",
    "model_v3.add(Dense(units=period_length))\n",
    "model_v3.add(Activation(activation_function))\n",
    "\n",
    "model_v3.compile(loss=\"mse\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_model(model=model_v3, X=X_train, Y=Y_train, epochs=300, \n",
    "            version=3, run_number=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Strategies\n",
    "In this section we implement a `Dropout()` regularization strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v3 = Sequential()\n",
    "model_v3.add(LSTM(\n",
    "    units=period_length,\n",
    "    batch_input_shape=(batch_size, number_of_periods, period_length),\n",
    "    input_shape=(number_of_periods, period_length),\n",
    "    return_sequences=True, stateful=False))\n",
    "\n",
    "#\n",
    "#  Implement a Dropout() here.\n",
    "#\n",
    "\n",
    "model_v3.add(LSTM(\n",
    "    units=period_length,\n",
    "    batch_input_shape=(batch_size, number_of_periods, period_length),\n",
    "    input_shape=(number_of_periods, period_length),\n",
    "    return_sequences=False, stateful=False))\n",
    "\n",
    "#\n",
    "#  Implement a Dropout() here too.\n",
    "#\n",
    "\n",
    "model_v3.add(Dense(units=period_length))\n",
    "model_v3.add(Activation(activation_function))\n",
    "\n",
    "model_v3.compile(loss=\"mse\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_model(model=model_v3, X=X_train, Y=Y_train, epochs=600, \n",
    "            version=3, run_number=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models\n",
    "After creating the model versions above, we now have to evaluate which model is performing best in our test data. In order to do that we will use three metrics: MSE, RMSE, and MAPE. MSE is used to compare the error rates of the model on each predicted week. RMSE and MAPE are computed for making the model results easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_set = np.concatenate((train_data, test_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, kind='series'):\n",
    "    \"\"\"\n",
    "    Uses Keras model.evaluate() method to compute\n",
    "    the MSE for all future weeks in period.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Keras trained model\n",
    "    \n",
    "    kind: str, default 'series'\n",
    "        Kind of evaluation to perform. If 'series', \n",
    "        then the model will perform an evaluation \n",
    "        over the complete series.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    evaluated_weeks: list\n",
    "        List of MSE values for each evaluated\n",
    "        test week.\n",
    "    \"\"\"\n",
    "    if kind == 'series':\n",
    "        predicted_weeks = []\n",
    "        for i in range(0, test_data.shape[1]):\n",
    "            input_series = combined_set[0:,i:i+76]\n",
    "            predicted_weeks.append(model.predict(input_series))\n",
    "\n",
    "        predicted_days = []\n",
    "        for week in predicted_weeks:\n",
    "            predicted_days += list(week[0])\n",
    "\n",
    "        return predicted_days\n",
    "    else:\n",
    "        evaluated_weeks = []\n",
    "        for i in range(0, test_data.shape[1]):\n",
    "            input_series = combined_set[0:,i:i+77]\n",
    "\n",
    "            X_test = input_series[0:,:-1].reshape(1, input_series.shape[1] - 1, 7)\n",
    "            Y_test = input_series[0:,-1:][0]\n",
    "\n",
    "            result = model.evaluate(x=X_test, y=Y_test, verbose=0)\n",
    "            evaluated_weeks.append(result)\n",
    "            \n",
    "            return evaluated_weeks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weekly_mse(series, model_name, color):\n",
    "    ax = pd.Series(series).plot(drawstyle=\"steps-post\",\n",
    "                                figsize=(14,4),\n",
    "                                linewidth=2,\n",
    "                                color=color,\n",
    "                                grid=True,\n",
    "                                label=model_name,\n",
    "                                alpha=0.7,\n",
    "                                title='Mean Squared Error (MSE) for Test Data (all models)'.format(\n",
    "                                       model_name))\n",
    "\n",
    "    y = [i for i in range(0, len(series))]\n",
    "    yint = range(min(y), math.ceil(max(y))+1)\n",
    "    plt.xticks(yint)\n",
    "\n",
    "    ax.set_xlabel(\"Predicted Week\")\n",
    "    ax.set_ylabel(\"MSE\")\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the weekly MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weekly_predictions(predicted_days, name, display_plot=True, \n",
    "                            variable='close'):\n",
    "    \n",
    "    combined = pd.concat([train, test])\n",
    "    \n",
    "    last_day = datetime.strptime(train['date'].max(), '%Y-%m-%d')\n",
    "    list_of_days = []\n",
    "    for days in range(1, len(predicted_days) + 1):\n",
    "        D = (last_day + timedelta(days=days)).strftime('%Y-%m-%d')\n",
    "        list_of_days.append(D)\n",
    "    \n",
    "    predicted = pd.DataFrame({\n",
    "        'date': list_of_days, \n",
    "        'close_point_relative_normalization': predicted_days \n",
    "    })\n",
    "    \n",
    "    combined['date'] = combined['date'].apply(\n",
    "                    lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "    \n",
    "    predicted['date'] = predicted['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "    \n",
    "    observed = combined[combined['date'] > train['date'].max()]\n",
    "    \n",
    "    predicted['iso_week'] = predicted['date'].apply(\n",
    "                            lambda x: x.strftime('%Y-%U'))\n",
    "    \n",
    "    predicted_close = predicted.groupby('iso_week').apply(\n",
    "                            lambda x: denormalize(observed, x))\n",
    "    \n",
    "    plot_two_series(observed, predicted_close, \n",
    "                    variable=variable,\n",
    "                    title='{}: Predictions per Week'.format(name))\n",
    "    \n",
    "    print('RMSE: {:.4f}'.format(\n",
    "        rmse(observed[variable][:-3], \n",
    "             predicted_close[variable])))\n",
    "\n",
    "    print('MAPE: {:.1f}%'.format(\n",
    "        mape(observed[variable][:-3], \n",
    "             predicted_close[variable])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate each one the models trained in this activity in sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_v0, model_v1, model_v2, model_v3]\n",
    "for i, M in enumerate(models):\n",
    "    predicted_days = evaluate_model(M, kind='other')\n",
    "    plot_weekly_predictions(predicted_days, 'model_v{}'.format(i), display_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first model outperformed all the other models. Take the opportunity and teak the values for the optimization techniques above and attempt to beat the performance of that model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
