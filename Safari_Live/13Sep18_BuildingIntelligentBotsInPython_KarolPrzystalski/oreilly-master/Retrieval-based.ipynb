{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building intelligent bots. Retrieval-based chatbots\n",
    "\n",
    "In this section we build a retrieval-based chatbot with Rasa. Before we go to this point, we go through a few NLP methods and word vectorization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP methods for NLU\n",
    "\n",
    "Let's take one of President Trump's speech and divide into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "file = open(\"trump.txt\", \"r\",encoding='utf-8') \n",
    "trump = file.read() \n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(trump)\n",
    "\n",
    "for span in doc.sents:\n",
    "    print(\"> \", span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have are able to divide it using SpaCy and get the part of speech of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for span in doc.sents:\n",
    "    for i in range(span.start, span.end):\n",
    "        token = doc[i]\n",
    "        print(i, token.text, token.pos_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A smaller example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = \"Broadcasting today, live from KrakÃ³w, on chatbots.\"\n",
    "\n",
    "doc = nlp(sample)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun chunks\n",
    "\n",
    "This NLP method is used to get the nouns from any sentene. It's important to understand what is the sentence about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(sample)\n",
    "for nc in doc.noun_chunks:\n",
    "    print(nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "NER is a NLP method where we get not the nouns or part of speech, but meanings of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(sample)\n",
    "for entity in doc.ents:\n",
    "    print(entity.label_, entity.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectorization\n",
    "\n",
    "Word vectorization is a process of preparing a vector representing each word. Gensim has an implementation of Word2Vec. We use a dimension of 100 and distance between two words in a sentence to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the vocabulary as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = list(model.wv.vocab)\n",
    "X = model[vocab]\n",
    "print(vocab[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train we just use the TSNE to reduce the dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw the words in a two-dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a more complex example and use a longer text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget http://mattmahoney.net/dc/text8.zip\n",
    "!unzip text8.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the sentences from the text as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "sentences = word2vec.Text8Corpus('text8',max_sentence_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's the time consuming part, load it and save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, size=100, window=5)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose only 100 words. It's easier to draw 100 words from the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "vocab = list(model.wv.vocab)\n",
    "vocab = random.sample(vocab,100)\n",
    "X = model[vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reduce the dimensionality of the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the chosen words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity measure through vectors\n",
    "\n",
    "SpaCy already has words vectorized and we can simply check the similarity between two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "doc1 = nlp(u\"Warsaw is the largest city in Poland.\")\n",
    "doc2 = nlp(u\"Crossaint is baked in France.\")\n",
    "doc3 = nlp(u\"An emu is a large bird.\")\n",
    "\n",
    "for doc in [doc1, doc2, doc3]:\n",
    "    for other_doc in [doc1, doc2, doc3]:\n",
    "        print(doc.similarity(other_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice example of word vectorization done by some researchers at Warsaw University: [Word2Vec](https://lamyiowce.github.io/word2viz/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-based chatbot\n",
    "\n",
    "In this section we use Rasa to build a very simple HR assistant bot. We can use Rasa as a server or use it directly from Python level. To start Rasa server you need to execute the following command:\n",
    "python3 -m rasa_nlu.server &\n",
    "It starts a server on default port 5000. You can test it using the request package. We should get the intent of the phrase `hi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_intent(sentence):\n",
    "    url = \"http://localhost:5000/parse\"\n",
    "    payload = {\"q\":sentence}\n",
    "    response = requests.get(url,params=payload)    \n",
    "    print(response.json())\n",
    "    intent = response.json()['intent']\n",
    "    if intent['confidence'] > 0.5: \n",
    "        return intent['name']\n",
    "    return response.json()\n",
    "\n",
    "get_intent(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To use Rasa from Python level you need to prepare a config file that contains the pipeline and the filename of examples used for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "{\n",
    "  \"pipeline\": \"spacy_sklearn\",\n",
    "  \"path\" : \".\",\n",
    "  \"data\" : \".anna.json\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "config_file = open(\"config.json\", \"w\")\n",
    "config_file.write(config)\n",
    "config_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The data file contains examples that are used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anna_common_examples = \"\"\"\n",
    "{\n",
    "  \"rasa_nlu_data\": {\n",
    "    \"entity_synonyms\": [\n",
    "      {\n",
    "        \"value\": \"candidate\",\n",
    "        \"synonyms\": [\"developer\", \"data scientist\"]\n",
    "      }\n",
    "    ],\n",
    "    \"common_examples\": [\n",
    "      {\n",
    "        \"text\": \"hey\", \n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"howdy\", \n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"hey there\",\n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"hello\", \n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"hi\", \n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"good morning\",\n",
    "        \"intent\": \"greet\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"good evening\",\n",
    "        \"intent\": \"greet\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"dear sir\",\n",
    "        \"intent\": \"greet\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"yes\", \n",
    "        \"intent\": \"affirm\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"yep\", \n",
    "        \"intent\": \"affirm\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"yeah\", \n",
    "        \"intent\": \"affirm\", \n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"indeed\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"that's right\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"ok\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"great\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"right, thank you\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"add candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"add candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": [\n",
    "            {\n",
    "      \"start\": 5,\n",
    "      \"end\": 13,\n",
    "      \"value\": \"candidate\",\n",
    "      \"entity\": \"candidate\"\n",
    "        }\n",
    "        ]\n",
    "      },         \n",
    "      {\n",
    "        \"text\": \"adding candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": [\n",
    "            {\n",
    "              \"start\": 8,\n",
    "              \"end\": 16,\n",
    "              \"value\": \"candidate\",\n",
    "              \"entity\": \"candidate\"\n",
    "            }        \n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"please add candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      },              \n",
    "      {\n",
    "        \"text\": \"please add new candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      },           \n",
    "      {\n",
    "        \"text\": \"we have new prescreening upcoming\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"we have a new candidate for prescreening\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      },         \n",
    "      {\n",
    "        \"text\": \"correct\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"great choice\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"sounds really good\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"bye\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"goodbye\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"good bye\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"stop\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"end\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"farewell\",\n",
    "        \"intent\": \"goodbye\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"Bye bye\",\n",
    "        \"intent\": \"goodbye\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"have a good one\",\n",
    "        \"intent\": \"goodbye\",\n",
    "        \"entities\": []\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "training_data = open(\"anna.json\", \"w\")\n",
    "training_data.write(anna_common_examples)\n",
    "training_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is straight forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rasa_nlu.converters import load_data\n",
    "from rasa_nlu.config import RasaNLUConfig\n",
    "from rasa_nlu.model import Trainer\n",
    "\n",
    "training_data = load_data('anna.json')\n",
    "trainer = Trainer(RasaNLUConfig(\"config.json\"))\n",
    "trainer.train(training_data)\n",
    "model_directory = trainer.persist('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the intent we use the parse method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rasa_nlu.model import Metadata, Interpreter\n",
    "\n",
    "interpreter = Interpreter.load(model_directory, RasaNLUConfig(\"config.json\"))\n",
    "\n",
    "interpreter.parse(u\"a new developer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 2\n",
    "\n",
    "Extend the training examples and add an intent `change_status` with entities: `passed` and `failed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anna_common_examples = \"\"\"\n",
    "{\n",
    "  \"rasa_nlu_data\": {\n",
    "    \"entity_synonyms\": [\n",
    "      {\n",
    "        \"value\": \"candidate\",\n",
    "        \"synonyms\": [\"developer\", \"data scientist\"]\n",
    "      }\n",
    "    ],\n",
    "    \"common_examples\": [\n",
    "      {\n",
    "        \"text\": \"hey\", \n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"howdy\", \n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"hey there\",\n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"hello\", \n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"hi\", \n",
    "        \"intent\": \"greet\", \n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"good morning\",\n",
    "        \"intent\": \"greet\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"good evening\",\n",
    "        \"intent\": \"greet\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"dear sir\",\n",
    "        \"intent\": \"greet\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"yes\", \n",
    "        \"intent\": \"affirm\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"yep\", \n",
    "        \"intent\": \"affirm\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"yeah\", \n",
    "        \"intent\": \"affirm\", \n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"indeed\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"that's right\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"ok\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"great\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"right, thank you\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"add candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"add candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": [\n",
    "            {\n",
    "      \"start\": 5,\n",
    "      \"end\": 13,\n",
    "      \"value\": \"candidate\",\n",
    "      \"entity\": \"candidate\"\n",
    "        }\n",
    "        ]\n",
    "      },         \n",
    "      {\n",
    "        \"text\": \"adding candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": [\n",
    "            {\n",
    "              \"start\": 8,\n",
    "      \"end\": 16,\n",
    "      \"value\": \"candidate\",\n",
    "      \"entity\": \"candidate\"\n",
    "        }        \n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"please add candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      },              \n",
    "      {\n",
    "        \"text\": \"please add new candidate\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      },           \n",
    "      {\n",
    "        \"text\": \"we have new prescreening upcoming\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"we have a new candidate for prescreening\",\n",
    "        \"intent\": \"candidate_add\",\n",
    "        \"entities\": []\n",
    "      },         \n",
    "      {\n",
    "        \"text\": \"correct\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"great choice\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"sounds really good\",\n",
    "        \"intent\": \"affirm\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"bye\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"goodbye\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"good bye\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"stop\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      }, \n",
    "      {\n",
    "        \"text\": \"end\", \n",
    "        \"intent\": \"goodbye\", \n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"farewell\",\n",
    "        \"intent\": \"goodbye\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"Bye bye\",\n",
    "        \"intent\": \"goodbye\",\n",
    "        \"entities\": []\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"have a good one\",\n",
    "        \"intent\": \"goodbye\",\n",
    "        \"entities\": []\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "training_data = open(\"anna.json\", \"w\")\n",
    "training_data.write(anna_common_examples)\n",
    "training_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rasa_nlu.converters import load_data\n",
    "from rasa_nlu.config import RasaNLUConfig\n",
    "from rasa_nlu.model import Trainer\n",
    "\n",
    "training_data = load_data('anna.json')\n",
    "trainer = Trainer(RasaNLUConfig(\"config.json\"))\n",
    "trainer.train(training_data)\n",
    "model_directory = trainer.persist('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rasa_nlu.model import Metadata, Interpreter\n",
    "\n",
    "interpreter = Interpreter.load(model_directory, RasaNLUConfig(\"config.json\"))\n",
    "\n",
    "interpreter.parse(u\"the developer didn't passed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
